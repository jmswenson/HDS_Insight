{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fnil\fcharset0 PTSerif-Regular;\f1\fnil\fcharset0 HelveticaNeue;\f2\fswiss\fcharset0 Helvetica;
}
{\colortbl;\red255\green255\blue255;\red38\green38\blue38;\red25\green25\blue25;\red0\green0\blue0;
\red251\green251\blue251;\red28\green28\blue28;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\fi390\sl300\sa390\partightenfactor0

\f0\b\fs24 \cf2 \expnd0\expndtw0\kerning0
I probably need to adjust the perplexity based on density. One way to generalize this approach would be do tsne on the control well and then get \'93gates\'94 based on that. Or tsne on the entire well.  \'93
\f1\fs26 \cf3 train a multivariate regressor to predict the map location from the input data.\'94\
\pard\pardeftab720\sl420\partightenfactor0

\f2\b0\fs36 \cf4 \cb5 \outl0\strokewidth0 \strokec4 A second feature of t-SNE is a tuneable parameter, \'93perplexity,\'94 which says (loosely) how to balance attention between local and global aspects of your data. The parameter is, in a sense, a guess about the number of close neighbors each point has. \'97 {\field{\*\fldinst{HYPERLINK "http://distill.pub/2016/misread-tsne/"}}{\fldrslt http://distill.pub/2016/misread-tsne/}} \'97 this is a really good source.
\f0\b\fs24 \cf2 \cb1 \outl0\strokewidth0 \
\pard\pardeftab720\fi390\sl300\sa390\partightenfactor0
\cf2 \
How should I set the perplexity in t-SNE?
\b0 \
The performance of t-SNE is fairly robust under different settings of the perplexity. The most appropriate value depends on the density of your data. Loosely speaking, one could say that a larger / denser dataset requires a larger perplexity. Typical values for the perplexity range between 5 and 50.\
\uc0\u8232 
\b What is perplexity anyway?
\b0 \
Perplexity is a measure for information that is defined as 2 to the power of the Shannon entropy. The perplexity of a fair die with k sides is equal to k. In t-SNE, the perplexity may be viewed as a knob that sets the number of effective nearest neighbors. It is comparable with the number of nearest neighbors k that is employed in many manifold learners.\
\
\pard\pardeftab720\sl300\partightenfactor0

\f1\fs26 \cf3 Once I have a t-SNE map, how can I embed incoming test points in that map?\
\
t-SNE learns a non-parametric mapping, which means that it does not learn an explicit function that maps data from the input space to the map. Therefore, it is not possible to embed test points in an existing map (although you could re-run t-SNE on the full dataset). 
\b \cf3 A potential approach to deal with this would be to train a multivariate regressor to predict the map location from the input data. Alternatively, you could also make such a regressor minimize the t-SNE loss directly, which is what I did in this paper.
\f0\b0\fs24 \cf2 \
\pard\pardeftab720\fi390\sl300\sa390\partightenfactor0
\cf2 \
From: {\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"}}{\fldrslt https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding}} :\
\pard\pardeftab720\sl320\sa140\partightenfactor0

\f2\fs28 \cf6 Given a set of n high-dimensional objects X1,\'85Xn, t-SNE first computes probabilities pij  that are proportional to the similarity of objects Xi and Xn, as follows:\
pj|i = Gaussian distribution of xi, xj divided by the sum of all Gaussian distributions of the data. So this will be a number between 0 and 1.\
}